{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-22T18:18:27.121996Z","iopub.execute_input":"2021-05-22T18:18:27.122385Z","iopub.status.idle":"2021-05-22T18:18:27.132045Z","shell.execute_reply.started":"2021-05-22T18:18:27.122307Z","shell.execute_reply":"2021-05-22T18:18:27.130897Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Setting Environment","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/NVIDIA/apex","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:18:27.138686Z","iopub.execute_input":"2021-05-22T18:18:27.138965Z","iopub.status.idle":"2021-05-22T18:18:29.711957Z","shell.execute_reply.started":"2021-05-22T18:18:27.138934Z","shell.execute_reply":"2021-05-22T18:18:29.711005Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'apex'...\nremote: Enumerating objects: 8042, done.\u001b[K\nremote: Counting objects: 100% (129/129), done.\u001b[K\nremote: Compressing objects: 100% (94/94), done.\u001b[K\nremote: Total 8042 (delta 61), reused 69 (delta 30), pack-reused 7913\u001b[K\nReceiving objects: 100% (8042/8042), 14.11 MiB | 19.19 MiB/s, done.\nResolving deltas: 100% (5460/5460), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:18:29.713724Z","iopub.execute_input":"2021-05-22T18:18:29.714103Z","iopub.status.idle":"2021-05-22T18:18:30.352840Z","shell.execute_reply.started":"2021-05-22T18:18:29.714061Z","shell.execute_reply":"2021-05-22T18:18:30.351918Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"__notebook_source__.ipynb  apex\n","output_type":"stream"}]},{"cell_type":"code","source":"cd apex","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:18:30.355085Z","iopub.execute_input":"2021-05-22T18:18:30.355430Z","iopub.status.idle":"2021-05-22T18:18:30.364315Z","shell.execute_reply.started":"2021-05-22T18:18:30.355389Z","shell.execute_reply":"2021-05-22T18:18:30.363349Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/apex\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:18:30.366280Z","iopub.execute_input":"2021-05-22T18:18:30.367059Z","iopub.status.idle":"2021-05-22T18:22:36.383384Z","shell.execute_reply.started":"2021-05-22T18:18:30.367019Z","shell.execute_reply":"2021-05-22T18:22:36.382214Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.7/site-packages/pip/_internal/commands/install.py:230: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n  cmdoptions.check_install_build_global(options)\nUsing pip 21.0.1 from /opt/conda/lib/python3.7/site-packages/pip (python 3.7)\nNon-user install because site-packages writeable\nCreated temporary directory: /tmp/pip-ephem-wheel-cache-7v2j427r\nCreated temporary directory: /tmp/pip-req-tracker-hq9j59ic\nInitialized build tracking at /tmp/pip-req-tracker-hq9j59ic\nCreated build tracker: /tmp/pip-req-tracker-hq9j59ic\nEntered build tracker: /tmp/pip-req-tracker-hq9j59ic\nCreated temporary directory: /tmp/pip-install-lwsh3js3\nProcessing /kaggle/working/apex\n  Created temporary directory: /tmp/pip-req-build-j0in6zbu\n  Added file:///kaggle/working/apex to build tracker '/tmp/pip-req-tracker-hq9j59ic'\n    Running setup.py (path:/tmp/pip-req-build-j0in6zbu/setup.py) egg_info for package from file:///kaggle/working/apex\n    Created temporary directory: /tmp/pip-pip-egg-info-6hic1yzf\n    Running command python setup.py egg_info\n\n\n    torch.__version__  = 1.7.0\n\n\n    running egg_info\n    creating /tmp/pip-pip-egg-info-6hic1yzf/apex.egg-info\n    writing /tmp/pip-pip-egg-info-6hic1yzf/apex.egg-info/PKG-INFO\n    writing dependency_links to /tmp/pip-pip-egg-info-6hic1yzf/apex.egg-info/dependency_links.txt\n    writing top-level names to /tmp/pip-pip-egg-info-6hic1yzf/apex.egg-info/top_level.txt\n    writing manifest file '/tmp/pip-pip-egg-info-6hic1yzf/apex.egg-info/SOURCES.txt'\n    writing manifest file '/tmp/pip-pip-egg-info-6hic1yzf/apex.egg-info/SOURCES.txt'\n    /tmp/pip-req-build-j0in6zbu/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n  Source in /tmp/pip-req-build-j0in6zbu has version 0.1, which satisfies requirement apex==0.1 from file:///kaggle/working/apex\n  Removed apex==0.1 from file:///kaggle/working/apex from build tracker '/tmp/pip-req-tracker-hq9j59ic'\nCreated temporary directory: /tmp/pip-unpack-81e206wa\nSkipping wheel build for apex, due to binaries being disabled for it.\nInstalling collected packages: apex\n  Created temporary directory: /tmp/pip-record-qinvlgen\n    Running command /opt/conda/bin/python3.7 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-j0in6zbu/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-j0in6zbu/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-qinvlgen/install-record.txt --single-version-externally-managed --compile --install-headers /opt/conda/include/python3.7m/apex\n\n\n    torch.__version__  = 1.7.0\n\n\n    /tmp/pip-req-build-j0in6zbu/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n\n    Compiling cuda extensions with\n    nvcc: NVIDIA (R) Cuda compiler driver\n    Copyright (c) 2005-2020 NVIDIA Corporation\n    Built on Wed_Jul_22_19:09:09_PDT_2020\n    Cuda compilation tools, release 11.0, V11.0.221\n    Build cuda_11.0_bu.TC445_37.28845127_0\n    from /usr/local/cuda/bin\n\n    running install\n    running build\n    running build_py\n    creating build\n    creating build/lib.linux-x86_64-3.7\n    creating build/lib.linux-x86_64-3.7/apex\n    copying apex/__init__.py -> build/lib.linux-x86_64-3.7/apex\n    creating build/lib.linux-x86_64-3.7/apex/fp16_utils\n    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.7/apex/fp16_utils\n    creating build/lib.linux-x86_64-3.7/apex/RNN\n    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.7/apex/RNN\n    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.7/apex/RNN\n    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.7/apex/RNN\n    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.7/apex/RNN\n    creating build/lib.linux-x86_64-3.7/apex/reparameterization\n    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.7/apex/reparameterization\n    creating build/lib.linux-x86_64-3.7/apex/optimizers\n    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.7/apex/optimizers\n    creating build/lib.linux-x86_64-3.7/apex/pyprof\n    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof\n    creating build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.7/apex/amp\n    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.7/apex/amp\n    creating build/lib.linux-x86_64-3.7/apex/parallel\n    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.7/apex/parallel\n    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.7/apex/parallel\n    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.7/apex/parallel\n    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.7/apex/parallel\n    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.7/apex/parallel\n    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.7/apex/parallel\n    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.7/apex/parallel\n    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.7/apex/parallel\n    creating build/lib.linux-x86_64-3.7/apex/contrib\n    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib\n    creating build/lib.linux-x86_64-3.7/apex/mlp\n    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.7/apex/mlp\n    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.7/apex/mlp\n    creating build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.7/apex/multi_tensor_apply\n    creating build/lib.linux-x86_64-3.7/apex/normalization\n    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.7/apex/normalization\n    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.7/apex/normalization\n    creating build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/prof\n    creating build/lib.linux-x86_64-3.7/apex/pyprof/parse\n    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/parse\n    creating build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.7/apex/pyprof/nvtx\n    creating build/lib.linux-x86_64-3.7/apex/amp/lists\n    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.7/apex/amp/lists\n    creating build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.7/apex/contrib/sparsity\n    creating build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.7/apex/contrib/optimizers\n    creating build/lib.linux-x86_64-3.7/apex/contrib/transducer\n    copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/transducer\n    copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-3.7/apex/contrib/transducer\n    creating build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.7/apex/contrib/xentropy\n    creating build/lib.linux-x86_64-3.7/apex/contrib/fmha\n    copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/fmha\n    copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-3.7/apex/contrib/fmha\n    creating build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn\n    creating build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n    copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n    copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/layer_norm\n    creating build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/groupbn\n    creating build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n    copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n    copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n    copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-3.7/apex/contrib/bottleneck\n    running build_ext\n    building 'apex_C' extension\n    creating /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7\n    creating /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc\n    Emitting ninja build file /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/build.ninja...\n    Compiling objects...\n    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n    [1/1] c++ -MMD -MF /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/flatten_unflatten.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/flatten_unflatten.cpp -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Parallel.h:149:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/flatten_unflatten.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n     #pragma omp parallel for if ((end - begin) >= grain_size)\n\n    In file included from /tmp/pip-req-build-j0in6zbu/csrc/flatten_unflatten.cpp:2:0:\n    /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n    /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n         return tensors[0].type();\n                                ^\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/flatten_unflatten.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/flatten_unflatten.o -L/opt/conda/lib/python3.7/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/apex_C.cpython-37m-x86_64-linux-gnu.so\n    building 'amp_C' extension\n    Emitting ninja build file /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/build.ninja...\n    Compiling objects...\n    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n    [1/11] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/multi_tensor_scale_kernel.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    [2/11] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/multi_tensor_sgd_kernel.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    [3/11] c++ -MMD -MF /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/amp_C_frontend.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/amp_C_frontend.cpp -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Parallel.h:149:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/amp_C_frontend.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n     #pragma omp parallel for if ((end - begin) >= grain_size)\n\n    [4/11] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/multi_tensor_l2norm_kernel.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    [5/11] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/multi_tensor_axpby_kernel.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    [6/11] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/multi_tensor_lamb_stage_1.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    [7/11] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/multi_tensor_lamb_stage_2.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    [8/11] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/multi_tensor_adam.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    [9/11] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/multi_tensor_adagrad.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    [10/11] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/multi_tensor_novograd.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    [11/11] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/multi_tensor_lamb.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/amp_C_frontend.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_sgd_kernel.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_scale_kernel.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_axpby_kernel.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_l2norm_kernel.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_1.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb_stage_2.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_adam.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_adagrad.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_novograd.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/multi_tensor_lamb.o -L/opt/conda/lib/python3.7/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.7/amp_C.cpython-37m-x86_64-linux-gnu.so\n    building 'syncbn' extension\n    Emitting ninja build file /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/build.ninja...\n    Compiling objects...\n    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n    [1/2] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/welford.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    [2/2] c++ -MMD -MF /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/syncbn.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/syncbn.cpp -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Parallel.h:149:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/syncbn.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n     #pragma omp parallel for if ((end - begin) >= grain_size)\n\n    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/syncbn.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/welford.o -L/opt/conda/lib/python3.7/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.7/syncbn.cpython-37m-x86_64-linux-gnu.so\n    building 'fused_layer_norm_cuda' extension\n    Emitting ninja build file /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/build.ninja...\n    Compiling objects...\n    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n    [1/2] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda_kernel.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    [2/2] c++ -MMD -MF /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Parallel.h:149:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n     #pragma omp parallel for if ((end - begin) >= grain_size)\n\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(input);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(input);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(gamma);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(beta);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(dout);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(mean);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(invvar);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(input);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(dout);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(mean);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(invvar);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(input);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(gamma);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/DeviceType.h:8:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Device.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/c10/core/Allocator.h:6,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:7,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                                              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n                                                                     ^~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n           ^~~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n       ^~~~~~~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n                                    ^~~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n                            ^~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n       CHECK_INPUT(beta);\n       ^~~~~~~~~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/layer_norm_cuda.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/layer_norm_cuda_kernel.o -L/opt/conda/lib/python3.7/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.7/fused_layer_norm_cuda.cpython-37m-x86_64-linux-gnu.so\n    building 'mlp_cuda' extension\n    Emitting ninja build file /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/build.ninja...\n    Compiling objects...\n    Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n    [1/2] c++ -MMD -MF /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/mlp.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n    cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Parallel.h:149:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n     #pragma omp parallel for if ((end - begin) >= grain_size)\n\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:57:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n       for (int i = 0; i < num_layers; i++) {\n                       ~~^~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n                                                                                 ^\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n                                                                       ^\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n                                                                        ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:67:59: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n       auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n                                                               ^\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:13:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp: In lambda function:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:69:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n                                                          ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n         const auto& the_type = TYPE;                                            \\\n                                ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:13:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n                                                            ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n       ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp: In lambda function:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < num_layers; i++) {\n                         ~~^~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n         auto result = mlp_fp<scalar_t>(\n              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp: In lambda function:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < num_layers; i++) {\n                         ~~^~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n         auto result = mlp_fp<scalar_t>(\n              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp: In lambda function:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < num_layers; i++) {\n                         ~~^~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n         auto result = mlp_fp<scalar_t>(\n              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:115:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n       for (int i = 0; i < num_layers; i++) {\n                       ~~^~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:120:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n       for (int i = 0; i < inputs.size(); i++) {\n                       ~~^~~~~~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:121:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n                                                                       ^\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:13:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp: In lambda function:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n                                                          ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n         const auto& the_type = TYPE;                                            \\\n                                ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:13:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n                                                            ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n                           ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp: In lambda function:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < num_layers; i++) {\n                         ~~^~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < inputs.size(); i++) {\n                         ~~^~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:138:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n                                                                                    ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:13:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n                                      ~~~~~~~~~~^~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n                                      ~~~~~~~~~~^~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n         auto result = mlp_bp<scalar_t>(\n              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp: In lambda function:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < num_layers; i++) {\n                         ~~^~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < inputs.size(); i++) {\n                         ~~^~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:138:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n                                                                                    ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:13:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n                                      ~~~~~~~~~~^~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n                                      ~~~~~~~~~~^~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n         auto result = mlp_bp<scalar_t>(\n              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp: In lambda function:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < num_layers; i++) {\n                         ~~^~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n         for (int i = 0; i < inputs.size(); i++) {\n                         ~~^~~~~~~~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:138:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n                                                                                    ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Tensor.h:3:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Context.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:9,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n       DeprecatedTypeProperties & type() const {\n                                  ^~~~\n    In file included from /opt/conda/lib/python3.7/site-packages/torch/include/ATen/ATen.h:13:0,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n                     from /opt/conda/lib/python3.7/site-packages/torch/include/torch/extension.h:4,\n                     from /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:1:\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n                                      ~~~~~~~~~~^~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:138:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n                                      ~~~~~~~~~~^~~~~~~~~\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n         auto result = mlp_bp<scalar_t>(\n              ^\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n         return __VA_ARGS__();                          \\\n                ^~~~~~~~~~~\n    /tmp/pip-req-build-j0in6zbu/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n       ^\n    [2/2] /usr/local/cuda/bin/nvcc -I/opt/conda/lib/python3.7/site-packages/torch/include -I/opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.7/site-packages/torch/include/TH -I/opt/conda/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.7m -c -c /tmp/pip-req-build-j0in6zbu/csrc/mlp_cuda.cu -o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n\n    /opt/conda/lib/python3.7/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n\n    g++ -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/mlp.o /tmp/pip-req-build-j0in6zbu/build/temp.linux-x86_64-3.7/csrc/mlp_cuda.o -L/opt/conda/lib/python3.7/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.7/mlp_cuda.cpython-37m-x86_64-linux-gnu.so\n    running install_lib\n    copying build/lib.linux-x86_64-3.7/fused_layer_norm_cuda.cpython-37m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.7/site-packages\n    copying build/lib.linux-x86_64-3.7/apex_C.cpython-37m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.7/site-packages\n    creating /opt/conda/lib/python3.7/site-packages/apex\n    creating /opt/conda/lib/python3.7/site-packages/apex/fp16_utils\n    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/fp16_utils\n    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/loss_scaler.py -> /opt/conda/lib/python3.7/site-packages/apex/fp16_utils\n    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/fp16util.py -> /opt/conda/lib/python3.7/site-packages/apex/fp16_utils\n    copying build/lib.linux-x86_64-3.7/apex/fp16_utils/fp16_optimizer.py -> /opt/conda/lib/python3.7/site-packages/apex/fp16_utils\n    copying build/lib.linux-x86_64-3.7/apex/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex\n    creating /opt/conda/lib/python3.7/site-packages/apex/RNN\n    copying build/lib.linux-x86_64-3.7/apex/RNN/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/RNN\n    copying build/lib.linux-x86_64-3.7/apex/RNN/models.py -> /opt/conda/lib/python3.7/site-packages/apex/RNN\n    copying build/lib.linux-x86_64-3.7/apex/RNN/cells.py -> /opt/conda/lib/python3.7/site-packages/apex/RNN\n    copying build/lib.linux-x86_64-3.7/apex/RNN/RNNBackend.py -> /opt/conda/lib/python3.7/site-packages/apex/RNN\n    creating /opt/conda/lib/python3.7/site-packages/apex/reparameterization\n    copying build/lib.linux-x86_64-3.7/apex/reparameterization/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/reparameterization\n    copying build/lib.linux-x86_64-3.7/apex/reparameterization/reparameterization.py -> /opt/conda/lib/python3.7/site-packages/apex/reparameterization\n    copying build/lib.linux-x86_64-3.7/apex/reparameterization/weight_norm.py -> /opt/conda/lib/python3.7/site-packages/apex/reparameterization\n    creating /opt/conda/lib/python3.7/site-packages/apex/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/optimizers/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_adagrad.py -> /opt/conda/lib/python3.7/site-packages/apex/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_lamb.py -> /opt/conda/lib/python3.7/site-packages/apex/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_novograd.py -> /opt/conda/lib/python3.7/site-packages/apex/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_sgd.py -> /opt/conda/lib/python3.7/site-packages/apex/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/optimizers/fused_adam.py -> /opt/conda/lib/python3.7/site-packages/apex/optimizers\n    creating /opt/conda/lib/python3.7/site-packages/apex/pyprof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof\n    creating /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/utility.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/pointwise.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/usage.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/blas.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/linear.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/index_slice_join_mutate.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/dropout.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/optim.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/randomSample.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/data.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/convert.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/base.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/reduction.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/embedding.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/prof.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/pooling.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/activation.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/misc.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/conv.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/softmax.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/normalization.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/recurrentCell.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/output.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/loss.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/prof/__main__.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof\n    creating /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/db.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/parse.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/kernel.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/nvvp.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/parse/__main__.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse\n    creating /opt/conda/lib/python3.7/site-packages/apex/pyprof/nvtx\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/nvtx/nvmarker.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/nvtx\n    copying build/lib.linux-x86_64-3.7/apex/pyprof/nvtx/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/pyprof/nvtx\n    creating /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/compat.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/rnn_compat.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/_amp_state.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/_initialize.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/frontend.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    creating /opt/conda/lib/python3.7/site-packages/apex/amp/lists\n    copying build/lib.linux-x86_64-3.7/apex/amp/lists/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/amp/lists\n    copying build/lib.linux-x86_64-3.7/apex/amp/lists/tensor_overrides.py -> /opt/conda/lib/python3.7/site-packages/apex/amp/lists\n    copying build/lib.linux-x86_64-3.7/apex/amp/lists/torch_overrides.py -> /opt/conda/lib/python3.7/site-packages/apex/amp/lists\n    copying build/lib.linux-x86_64-3.7/apex/amp/lists/functional_overrides.py -> /opt/conda/lib/python3.7/site-packages/apex/amp/lists\n    copying build/lib.linux-x86_64-3.7/apex/amp/opt.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/wrap.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/amp.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/utils.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/handle.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/_process_optimizer.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/__version__.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    copying build/lib.linux-x86_64-3.7/apex/amp/scaler.py -> /opt/conda/lib/python3.7/site-packages/apex/amp\n    creating /opt/conda/lib/python3.7/site-packages/apex/parallel\n    copying build/lib.linux-x86_64-3.7/apex/parallel/LARC.py -> /opt/conda/lib/python3.7/site-packages/apex/parallel\n    copying build/lib.linux-x86_64-3.7/apex/parallel/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/parallel\n    copying build/lib.linux-x86_64-3.7/apex/parallel/optimized_sync_batchnorm.py -> /opt/conda/lib/python3.7/site-packages/apex/parallel\n    copying build/lib.linux-x86_64-3.7/apex/parallel/sync_batchnorm.py -> /opt/conda/lib/python3.7/site-packages/apex/parallel\n    copying build/lib.linux-x86_64-3.7/apex/parallel/multiproc.py -> /opt/conda/lib/python3.7/site-packages/apex/parallel\n    copying build/lib.linux-x86_64-3.7/apex/parallel/sync_batchnorm_kernel.py -> /opt/conda/lib/python3.7/site-packages/apex/parallel\n    copying build/lib.linux-x86_64-3.7/apex/parallel/optimized_sync_batchnorm_kernel.py -> /opt/conda/lib/python3.7/site-packages/apex/parallel\n    copying build/lib.linux-x86_64-3.7/apex/parallel/distributed.py -> /opt/conda/lib/python3.7/site-packages/apex/parallel\n    creating /opt/conda/lib/python3.7/site-packages/apex/contrib\n    creating /opt/conda/lib/python3.7/site-packages/apex/contrib/sparsity\n    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/sparsity\n    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/asp.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/sparsity\n    copying build/lib.linux-x86_64-3.7/apex/contrib/sparsity/sparse_masklib.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/sparsity\n    copying build/lib.linux-x86_64-3.7/apex/contrib/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib\n    creating /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_lamb.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fp16_optimizer.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_lamb.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_sgd.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/fused_adam.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers\n    copying build/lib.linux-x86_64-3.7/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers\n    creating /opt/conda/lib/python3.7/site-packages/apex/contrib/transducer\n    copying build/lib.linux-x86_64-3.7/apex/contrib/transducer/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/transducer\n    copying build/lib.linux-x86_64-3.7/apex/contrib/transducer/transducer.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/transducer\n    creating /opt/conda/lib/python3.7/site-packages/apex/contrib/xentropy\n    copying build/lib.linux-x86_64-3.7/apex/contrib/xentropy/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/xentropy\n    copying build/lib.linux-x86_64-3.7/apex/contrib/xentropy/softmax_xentropy.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/xentropy\n    creating /opt/conda/lib/python3.7/site-packages/apex/contrib/fmha\n    copying build/lib.linux-x86_64-3.7/apex/contrib/fmha/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/fmha\n    copying build/lib.linux-x86_64-3.7/apex/contrib/fmha/fmha.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/fmha\n    creating /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/self_multihead_attn.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn\n    creating /opt/conda/lib/python3.7/site-packages/apex/contrib/layer_norm\n    copying build/lib.linux-x86_64-3.7/apex/contrib/layer_norm/layer_norm.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/layer_norm\n    copying build/lib.linux-x86_64-3.7/apex/contrib/layer_norm/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/layer_norm\n    creating /opt/conda/lib/python3.7/site-packages/apex/contrib/groupbn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/groupbn/batch_norm.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/groupbn\n    copying build/lib.linux-x86_64-3.7/apex/contrib/groupbn/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/groupbn\n    creating /opt/conda/lib/python3.7/site-packages/apex/contrib/bottleneck\n    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/test.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/bottleneck\n    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/bottleneck\n    copying build/lib.linux-x86_64-3.7/apex/contrib/bottleneck/bottleneck.py -> /opt/conda/lib/python3.7/site-packages/apex/contrib/bottleneck\n    creating /opt/conda/lib/python3.7/site-packages/apex/mlp\n    copying build/lib.linux-x86_64-3.7/apex/mlp/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/mlp\n    copying build/lib.linux-x86_64-3.7/apex/mlp/mlp.py -> /opt/conda/lib/python3.7/site-packages/apex/mlp\n    creating /opt/conda/lib/python3.7/site-packages/apex/multi_tensor_apply\n    copying build/lib.linux-x86_64-3.7/apex/multi_tensor_apply/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/multi_tensor_apply\n    copying build/lib.linux-x86_64-3.7/apex/multi_tensor_apply/multi_tensor_apply.py -> /opt/conda/lib/python3.7/site-packages/apex/multi_tensor_apply\n    creating /opt/conda/lib/python3.7/site-packages/apex/normalization\n    copying build/lib.linux-x86_64-3.7/apex/normalization/__init__.py -> /opt/conda/lib/python3.7/site-packages/apex/normalization\n    copying build/lib.linux-x86_64-3.7/apex/normalization/fused_layer_norm.py -> /opt/conda/lib/python3.7/site-packages/apex/normalization\n    copying build/lib.linux-x86_64-3.7/mlp_cuda.cpython-37m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.7/site-packages\n    copying build/lib.linux-x86_64-3.7/amp_C.cpython-37m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.7/site-packages\n    copying build/lib.linux-x86_64-3.7/syncbn.cpython-37m-x86_64-linux-gnu.so -> /opt/conda/lib/python3.7/site-packages\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/fp16_utils/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/RNN/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/RNN/models.py to models.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/RNN/cells.py to cells.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/reparameterization/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/optimizers/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/utility.py to utility.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/usage.py to usage.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/blas.py to blas.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/linear.py to linear.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/dropout.py to dropout.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/optim.py to optim.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/data.py to data.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/convert.py to convert.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/base.py to base.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/reduction.py to reduction.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/embedding.py to embedding.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/prof.py to prof.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/pooling.py to pooling.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/activation.py to activation.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/misc.py to misc.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/conv.py to conv.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/softmax.py to softmax.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/normalization.py to normalization.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/output.py to output.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/loss.py to loss.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/prof/__main__.py to __main__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse/db.py to db.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse/parse.py to parse.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse/kernel.py to kernel.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/parse/__main__.py to __main__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/compat.py to compat.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/_amp_state.py to _amp_state.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/_initialize.py to _initialize.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/frontend.py to frontend.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/lists/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/opt.py to opt.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/wrap.py to wrap.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/amp.py to amp.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/utils.py to utils.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/handle.py to handle.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/__version__.py to __version__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/amp/scaler.py to scaler.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/parallel/LARC.py to LARC.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/parallel/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/parallel/multiproc.py to multiproc.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/parallel/distributed.py to distributed.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/sparsity/asp.py to asp.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/transducer/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/transducer/transducer.py to transducer.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/fmha/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/fmha/fmha.py to fmha.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/layer_norm/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/bottleneck/test.py to test.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/bottleneck/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/mlp/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/mlp/mlp.py to mlp.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/normalization/__init__.py to __init__.cpython-37.pyc\n    byte-compiling /opt/conda/lib/python3.7/site-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-37.pyc\n    running install_egg_info\n    running egg_info\n    creating apex.egg-info\n    writing apex.egg-info/PKG-INFO\n    writing dependency_links to apex.egg-info/dependency_links.txt\n    writing top-level names to apex.egg-info/top_level.txt\n    writing manifest file 'apex.egg-info/SOURCES.txt'\n    writing manifest file 'apex.egg-info/SOURCES.txt'\n    Copying apex.egg-info to /opt/conda/lib/python3.7/site-packages/apex-0.1-py3.7.egg-info\n    running install_scripts\n    writing list of installed files to '/tmp/pip-record-qinvlgen/install-record.txt'\n    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\nSuccessfully installed apex-0.1\nRemoved build tracker: '/tmp/pip-req-tracker-hq9j59ic'\n","output_type":"stream"}]},{"cell_type":"code","source":"cd ..","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:36.384809Z","iopub.execute_input":"2021-05-22T18:22:36.385127Z","iopub.status.idle":"2021-05-22T18:22:36.395061Z","shell.execute_reply.started":"2021-05-22T18:22:36.385096Z","shell.execute_reply":"2021-05-22T18:22:36.394072Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nsys.path.append('./apex')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:36.396454Z","iopub.execute_input":"2021-05-22T18:22:36.396768Z","iopub.status.idle":"2021-05-22T18:22:36.412405Z","shell.execute_reply.started":"2021-05-22T18:22:36.396741Z","shell.execute_reply":"2021-05-22T18:22:36.411362Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Utils Functions","metadata":{}},{"cell_type":"code","source":"import torch\n\nSEED = 1111\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:36.413926Z","iopub.execute_input":"2021-05-22T18:22:36.414320Z","iopub.status.idle":"2021-05-22T18:22:36.738700Z","shell.execute_reply.started":"2021-05-22T18:22:36.414279Z","shell.execute_reply":"2021-05-22T18:22:36.737913Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:36.741101Z","iopub.execute_input":"2021-05-22T18:22:36.741342Z","iopub.status.idle":"2021-05-22T18:22:39.816045Z","shell.execute_reply.started":"2021-05-22T18:22:36.741317Z","shell.execute_reply":"2021-05-22T18:22:39.815184Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6079a28c24354afcadb4c1ad4ef4b1f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83c4431e94c34060b11cae70b1581b64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30add5dacf724eed885b3bfdfa8b6cbf"}},"metadata":{}}]},{"cell_type":"code","source":"len(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:39.818047Z","iopub.execute_input":"2021-05-22T18:22:39.818404Z","iopub.status.idle":"2021-05-22T18:22:39.825859Z","shell.execute_reply.started":"2021-05-22T18:22:39.818366Z","shell.execute_reply":"2021-05-22T18:22:39.824809Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"30522"},"metadata":{}}]},{"cell_type":"code","source":"tokens = tokenizer.tokenize('Heyy There!! See some boys are playing in rain')\n\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:39.827352Z","iopub.execute_input":"2021-05-22T18:22:39.827736Z","iopub.status.idle":"2021-05-22T18:22:39.837378Z","shell.execute_reply.started":"2021-05-22T18:22:39.827697Z","shell.execute_reply":"2021-05-22T18:22:39.836361Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['hey', '##y', 'there', '!', '!', 'see', 'some', 'boys', 'are', 'playing', 'in', 'rain']\n","output_type":"stream"}]},{"cell_type":"code","source":"indexes = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(indexes)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:39.839006Z","iopub.execute_input":"2021-05-22T18:22:39.839699Z","iopub.status.idle":"2021-05-22T18:22:39.846375Z","shell.execute_reply.started":"2021-05-22T18:22:39.839658Z","shell.execute_reply":"2021-05-22T18:22:39.845211Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[4931, 2100, 2045, 999, 999, 2156, 2070, 3337, 2024, 2652, 1999, 4542]\n","output_type":"stream"}]},{"cell_type":"code","source":"init_token = tokenizer.cls_token\neos_token = tokenizer.sep_token\npad_token = tokenizer.pad_token\nunk_token = tokenizer.unk_token\n\nprint(init_token, eos_token, pad_token, unk_token)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:39.847826Z","iopub.execute_input":"2021-05-22T18:22:39.848458Z","iopub.status.idle":"2021-05-22T18:22:39.856324Z","shell.execute_reply.started":"2021-05-22T18:22:39.848417Z","shell.execute_reply":"2021-05-22T18:22:39.855337Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[CLS] [SEP] [PAD] [UNK]\n","output_type":"stream"}]},{"cell_type":"code","source":"init_token_idx = tokenizer.cls_token_id\neos_token_idx = tokenizer.sep_token_id\npad_token_idx = tokenizer.pad_token_id\nunk_token_idx = tokenizer.unk_token_id\n\nprint(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:39.857905Z","iopub.execute_input":"2021-05-22T18:22:39.858605Z","iopub.status.idle":"2021-05-22T18:22:39.865416Z","shell.execute_reply.started":"2021-05-22T18:22:39.858568Z","shell.execute_reply":"2021-05-22T18:22:39.864368Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"101 102 0 100\n","output_type":"stream"}]},{"cell_type":"code","source":"max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n\nprint(max_input_length)\n\nmax_input_length = 255","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:39.866788Z","iopub.execute_input":"2021-05-22T18:22:39.867411Z","iopub.status.idle":"2021-05-22T18:22:39.875804Z","shell.execute_reply.started":"2021-05-22T18:22:39.867372Z","shell.execute_reply":"2021-05-22T18:22:39.874848Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"512\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_bert(sentence):\n    tokens = tokenizer.tokenize(sentence) \n    return tokens","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:39.877189Z","iopub.execute_input":"2021-05-22T18:22:39.877717Z","iopub.status.idle":"2021-05-22T18:22:39.884514Z","shell.execute_reply.started":"2021-05-22T18:22:39.877674Z","shell.execute_reply":"2021-05-22T18:22:39.883641Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def split_and_cut(sentence):\n    tokens = sentence.strip().split(\" \")\n    tokens = tokens[:max_input_length-1]\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:39.885989Z","iopub.execute_input":"2021-05-22T18:22:39.886398Z","iopub.status.idle":"2021-05-22T18:22:39.894256Z","shell.execute_reply.started":"2021-05-22T18:22:39.886361Z","shell.execute_reply":"2021-05-22T18:22:39.893338Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def trim_sentence(sent):\n    try:\n        sent = sent.split()\n        sent = sent[:128]\n        return \" \".join(sent)\n    except:\n        return sent","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:39.895637Z","iopub.execute_input":"2021-05-22T18:22:39.896302Z","iopub.status.idle":"2021-05-22T18:22:39.904166Z","shell.execute_reply.started":"2021-05-22T18:22:39.896263Z","shell.execute_reply":"2021-05-22T18:22:39.903403Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Download Dataset","metadata":{}},{"cell_type":"code","source":"!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:39.905621Z","iopub.execute_input":"2021-05-22T18:22:39.906075Z","iopub.status.idle":"2021-05-22T18:22:51.266832Z","shell.execute_reply.started":"2021-05-22T18:22:39.906039Z","shell.execute_reply":"2021-05-22T18:22:51.265794Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"--2021-05-22 18:22:40--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 94550081 (90M) [application/zip]\nSaving to: ‘snli_1.0.zip’\n\nsnli_1.0.zip        100%[===================>]  90.17M  3.36MB/s    in 11s     \n\n2021-05-22 18:22:51 (8.58 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from zipfile import ZipFile\n  \n# specifying the zip file name\nfile_name = \"snli_1.0.zip\"\n  \n# opening the zip file in READ mode\nwith ZipFile(file_name, 'r') as zip:\n    # printing all the contents of the zip file\n    zip.printdir()\n  \n    # extracting all the files\n    print('Extracting all the files now...')\n    zip.extractall()\n    print('Done!')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:51.269742Z","iopub.execute_input":"2021-05-22T18:22:51.270159Z","iopub.status.idle":"2021-05-22T18:22:54.896800Z","shell.execute_reply.started":"2021-05-22T18:22:51.270112Z","shell.execute_reply":"2021-05-22T18:22:54.895335Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"File Name                                             Modified             Size\nsnli_1.0/                                      2015-08-29 08:57:10            0\nsnli_1.0/.DS_Store                             2015-08-29 08:57:16         6148\n__MACOSX/                                      2015-08-29 09:00:04            0\n__MACOSX/snli_1.0/                             2015-08-29 09:00:04            0\n__MACOSX/snli_1.0/._.DS_Store                  2015-08-29 08:57:16          120\n                                 2015-05-21 16:21:08            0\n                      2015-05-21 16:21:08       340709\nsnli_1.0/README.txt                            2015-08-29 08:59:48         5828\n__MACOSX/snli_1.0/._README.txt                 2015-08-29 08:59:48          171\nsnli_1.0/snli_1.0_dev.jsonl                    2015-08-17 10:34:22      9745714\nsnli_1.0/snli_1.0_dev.txt                      2015-08-17 10:34:24      7565773\nsnli_1.0/snli_1.0_test.jsonl                   2015-08-17 10:34:26      9730457\nsnli_1.0/snli_1.0_test.txt                     2015-08-17 10:34:28      7550390\nsnli_1.0/snli_1.0_train.jsonl                  2015-08-17 10:34:52    487457790\nsnli_1.0/snli_1.0_train.txt                    2015-08-17 10:35:12    375697923\n__MACOSX/._snli_1.0                            2015-08-29 08:57:10          120\nExtracting all the files now...\nDone!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prepare Dataset","metadata":{}},{"cell_type":"code","source":"def get_sent1_token_type(sent):\n    try:\n        return [0]* len(sent)\n    except:\n        return []\n\ndef get_sent2_token_type(sent):\n    try:\n        return [1]* len(sent)\n    except:\n        return []\n    \ndef combine_seq(seq):\n    return \" \".join(seq)\n\ndef combine_mask(mask):\n    mask = [str(m) for m in mask]\n    return \" \".join(mask)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:54.898199Z","iopub.execute_input":"2021-05-22T18:22:54.898549Z","iopub.status.idle":"2021-05-22T18:22:54.904679Z","shell.execute_reply.started":"2021-05-22T18:22:54.898514Z","shell.execute_reply":"2021-05-22T18:22:54.903657Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv('snli_1.0/snli_1.0_train.txt', sep='\\t')\ndf_dev = pd.read_csv('snli_1.0/snli_1.0_dev.txt', sep='\\t')\ndf_test = pd.read_csv('snli_1.0/snli_1.0_test.txt', sep='\\t')\n\ndf_train = df_train[['gold_label','sentence1','sentence2']]\ndf_dev = df_dev[['gold_label','sentence1','sentence2']]\ndf_test = df_test[['gold_label','sentence1','sentence2']]\n\ndf_train = df_train[:80000]\ndf_dev = df_train[:8000]\ndf_test = df_train[:8000]\n\n\ndf_train['sentence1'] = df_train['sentence1'].apply(trim_sentence)\ndf_train['sentence2'] = df_train['sentence2'].apply(trim_sentence)\ndf_dev['sentence1'] = df_dev['sentence1'].apply(trim_sentence)\ndf_dev['sentence2'] = df_dev['sentence2'].apply(trim_sentence)\ndf_test['sentence1'] = df_test['sentence1'].apply(trim_sentence)\ndf_test['sentence2'] = df_test['sentence2'].apply(trim_sentence)\n\ndf_train['sent1'] = '[CLS] ' + df_train['sentence1'] + ' [SEP] '\ndf_train['sent2'] = df_train['sentence2'] + ' [SEP]'\ndf_dev['sent1'] = '[CLS] ' + df_dev['sentence1'] + ' [SEP] '\ndf_dev['sent2'] = df_dev['sentence2'] + ' [SEP]'\ndf_test['sent1'] = '[CLS] ' + df_test['sentence1'] + ' [SEP] '\ndf_test['sent2'] = df_test['sentence2'] + ' [SEP]'\n\ndf_train['sent1_t'] = df_train['sent1'].apply(tokenize_bert)\ndf_train['sent2_t'] = df_train['sent2'].apply(tokenize_bert)\ndf_dev['sent1_t'] = df_dev['sent1'].apply(tokenize_bert)\ndf_dev['sent2_t'] = df_dev['sent2'].apply(tokenize_bert)\ndf_test['sent1_t'] = df_test['sent1'].apply(tokenize_bert)\ndf_test['sent2_t'] = df_test['sent2'].apply(tokenize_bert)\n\ndf_train['sent1_token_type'] = df_train['sent1_t'].apply(get_sent1_token_type)\ndf_train['sent2_token_type'] = df_train['sent2_t'].apply(get_sent2_token_type)\ndf_dev['sent1_token_type'] = df_dev['sent1_t'].apply(get_sent1_token_type)\ndf_dev['sent2_token_type'] = df_dev['sent2_t'].apply(get_sent2_token_type)\ndf_test['sent1_token_type'] = df_test['sent1_t'].apply(get_sent1_token_type)\ndf_test['sent2_token_type'] = df_test['sent2_t'].apply(get_sent2_token_type)\n\ndf_train['sequence'] = df_train['sent1_t'] + df_train['sent2_t']\ndf_dev['sequence'] = df_dev['sent1_t'] + df_dev['sent2_t']\ndf_test['sequence'] = df_test['sent1_t'] + df_test['sent2_t']\n\n\ndf_train['attention_mask'] = df_train['sequence'].apply(get_sent2_token_type)\ndf_dev['attention_mask'] = df_dev['sequence'].apply(get_sent2_token_type)\ndf_test['attention_mask'] = df_test['sequence'].apply(get_sent2_token_type)\n\ndf_train['token_type'] = df_train['sent1_token_type'] + df_train['sent2_token_type']\ndf_dev['token_type'] = df_dev['sent1_token_type'] + df_dev['sent2_token_type']\ndf_test['token_type'] = df_test['sent1_token_type'] + df_test['sent2_token_type']\n\ndf_train['sequence'] = df_train['sequence'].apply(combine_seq)\ndf_dev['sequence'] = df_dev['sequence'].apply(combine_seq)\ndf_test['sequence'] = df_test['sequence'].apply(combine_seq)\n\ndf_train['attention_mask'] = df_train['attention_mask'].apply(combine_mask)\ndf_dev['attention_mask'] = df_dev['attention_mask'].apply(combine_mask)\ndf_test['attention_mask'] = df_test['attention_mask'].apply(combine_mask)\n\ndf_train['token_type'] = df_train['token_type'].apply(combine_mask)\ndf_dev['token_type'] = df_dev['token_type'].apply(combine_mask)\ndf_test['token_type'] = df_test['token_type'].apply(combine_mask)\n\ndf_train = df_train[['gold_label', 'sequence', 'attention_mask', 'token_type']]\ndf_dev = df_dev[['gold_label', 'sequence', 'attention_mask', 'token_type']]\ndf_test = df_test[['gold_label', 'sequence', 'attention_mask', 'token_type']]\n\n\n\ndf_train = df_train.loc[df_train['gold_label'].isin(['entailment','contradiction','neutral'])]\ndf_dev = df_dev.loc[df_dev['gold_label'].isin(['entailment','contradiction','neutral'])]\ndf_test = df_test.loc[df_test['gold_label'].isin(['entailment','contradiction','neutral'])]\n\n\n\ndf_train.to_csv('snli_1.0/snli_1.0_train.csv', index=False)\ndf_dev.to_csv('snli_1.0/snli_1.0_dev.csv', index=False)\ndf_test.to_csv('snli_1.0/snli_1.0_test.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:22:54.906188Z","iopub.execute_input":"2021-05-22T18:22:54.906704Z","iopub.status.idle":"2021-05-22T18:24:05.494888Z","shell.execute_reply.started":"2021-05-22T18:22:54.906664Z","shell.execute_reply":"2021-05-22T18:24:05.494039Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:59: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:62: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:63: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:66: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:05.496222Z","iopub.execute_input":"2021-05-22T18:24:05.496577Z","iopub.status.idle":"2021-05-22T18:24:05.510969Z","shell.execute_reply.started":"2021-05-22T18:24:05.496540Z","shell.execute_reply":"2021-05-22T18:24:05.509989Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"      gold_label                                           sequence  \\\n0        neutral  [CLS] a person on a horse jumps over a broken ...   \n1  contradiction  [CLS] a person on a horse jumps over a broken ...   \n2     entailment  [CLS] a person on a horse jumps over a broken ...   \n3        neutral  [CLS] children smiling and waving at camera [S...   \n4     entailment  [CLS] children smiling and waving at camera [S...   \n\n                                      attention_mask  \\\n0  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1   \n1  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...   \n2    1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1   \n3                      1 1 1 1 1 1 1 1 1 1 1 1 1 1 1   \n4                          1 1 1 1 1 1 1 1 1 1 1 1 1   \n\n                                          token_type  \n0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1  \n1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 ...  \n2    0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1  \n3                      0 0 0 0 0 0 0 0 1 1 1 1 1 1 1  \n4                          0 0 0 0 0 0 0 0 1 1 1 1 1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gold_label</th>\n      <th>sequence</th>\n      <th>attention_mask</th>\n      <th>token_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>[CLS] a person on a horse jumps over a broken ...</td>\n      <td>1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</td>\n      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>contradiction</td>\n      <td>[CLS] a person on a horse jumps over a broken ...</td>\n      <td>1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...</td>\n      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>entailment</td>\n      <td>[CLS] a person on a horse jumps over a broken ...</td>\n      <td>1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</td>\n      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>neutral</td>\n      <td>[CLS] children smiling and waving at camera [S...</td>\n      <td>1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</td>\n      <td>0 0 0 0 0 0 0 0 1 1 1 1 1 1 1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>entailment</td>\n      <td>[CLS] children smiling and waving at camera [S...</td>\n      <td>1 1 1 1 1 1 1 1 1 1 1 1 1</td>\n      <td>0 0 0 0 0 0 0 0 1 1 1 1 1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(df_train.iloc[0]['sequence'])\nprint(df_train.iloc[0]['attention_mask'])\nprint(df_train.iloc[0]['token_type'])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:05.516834Z","iopub.execute_input":"2021-05-22T18:24:05.517130Z","iopub.status.idle":"2021-05-22T18:24:05.523940Z","shell.execute_reply.started":"2021-05-22T18:24:05.517103Z","shell.execute_reply":"2021-05-22T18:24:05.523027Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[CLS] a person on a horse jumps over a broken down airplane . [SEP] a person is training his horse for a competition . [SEP]\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train['gold_label'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:05.526700Z","iopub.execute_input":"2021-05-22T18:24:05.527483Z","iopub.status.idle":"2021-05-22T18:24:05.546547Z","shell.execute_reply.started":"2021-05-22T18:24:05.527288Z","shell.execute_reply":"2021-05-22T18:24:05.545653Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"array(['neutral', 'contradiction', 'entailment'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:05.547967Z","iopub.execute_input":"2021-05-22T18:24:05.548321Z","iopub.status.idle":"2021-05-22T18:24:05.553413Z","shell.execute_reply.started":"2021-05-22T18:24:05.548287Z","shell.execute_reply":"2021-05-22T18:24:05.552328Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df_dev.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:05.555009Z","iopub.execute_input":"2021-05-22T18:24:05.555672Z","iopub.status.idle":"2021-05-22T18:24:05.570176Z","shell.execute_reply.started":"2021-05-22T18:24:05.555528Z","shell.execute_reply":"2021-05-22T18:24:05.569277Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"      gold_label  \\\n0        neutral   \n1  contradiction   \n2     entailment   \n\n                                                                                                                          sequence  \\\n0     [CLS] a person on a horse jumps over a broken down airplane . [SEP] a person is training his horse for a competition . [SEP]   \n1  [CLS] a person on a horse jumps over a broken down airplane . [SEP] a person is at a diner , ordering an om ##ele ##tte . [SEP]   \n2                    [CLS] a person on a horse jumps over a broken down airplane . [SEP] a person is outdoors , on a horse . [SEP]   \n\n                                            attention_mask  \\\n0        1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1   \n1  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1   \n2          1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1   \n\n                                                token_type  \n0        0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1  \n1  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1  \n2          0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gold_label</th>\n      <th>sequence</th>\n      <th>attention_mask</th>\n      <th>token_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>[CLS] a person on a horse jumps over a broken down airplane . [SEP] a person is training his horse for a competition . [SEP]</td>\n      <td>1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</td>\n      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>contradiction</td>\n      <td>[CLS] a person on a horse jumps over a broken down airplane . [SEP] a person is at a diner , ordering an om ##ele ##tte . [SEP]</td>\n      <td>1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</td>\n      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>entailment</td>\n      <td>[CLS] a person on a horse jumps over a broken down airplane . [SEP] a person is outdoors , on a horse . [SEP]</td>\n      <td>1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</td>\n      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def convert_to_int(tok_ids):\n    tok_ids = [int(x) for x in tok_ids]\n    return tok_ids","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:05.571467Z","iopub.execute_input":"2021-05-22T18:24:05.572040Z","iopub.status.idle":"2021-05-22T18:24:05.582200Z","shell.execute_reply.started":"2021-05-22T18:24:05.571999Z","shell.execute_reply":"2021-05-22T18:24:05.581378Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from torchtext import data\n\nTEXT = data.Field(batch_first = True,\n                  use_vocab = False,\n                  tokenize = split_and_cut,\n                  preprocessing = tokenizer.convert_tokens_to_ids,\n                  pad_token = pad_token_idx,\n                  unk_token = unk_token_idx)\n\nLABEL = data.LabelField()\n\nATTENTION = data.Field(batch_first = True,\n                  use_vocab = False,\n                  tokenize = split_and_cut,\n                  preprocessing = convert_to_int,\n                  pad_token = pad_token_idx)\n\nTTYPE = data.Field(batch_first = True,\n                  use_vocab = False,\n                  tokenize = split_and_cut,\n                  preprocessing = convert_to_int,\n                  pad_token = 1)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:05.584732Z","iopub.execute_input":"2021-05-22T18:24:05.585410Z","iopub.status.idle":"2021-05-22T18:24:05.643159Z","shell.execute_reply.started":"2021-05-22T18:24:05.585369Z","shell.execute_reply":"2021-05-22T18:24:05.642310Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"fields = [('label', LABEL), ('sequence', TEXT), ('attention_mask', ATTENTION), ('token_type', TTYPE)]\n\ntrain_data, valid_data, test_data = data.TabularDataset.splits(\n                                        path = 'snli_1.0',\n                                        train = 'snli_1.0_train.csv',\n                                        validation = 'snli_1.0_dev.csv',\n                                        test = 'snli_1.0_test.csv',\n                                        format = 'csv',\n                                        fields = fields,\n                                        skip_header = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:05.644341Z","iopub.execute_input":"2021-05-22T18:24:05.644689Z","iopub.status.idle":"2021-05-22T18:24:11.396943Z","shell.execute_reply.started":"2021-05-22T18:24:05.644654Z","shell.execute_reply":"2021-05-22T18:24:11.396034Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Number of training data: {len(train_data)}\")\nprint(f\"Number of validation data: {len(valid_data)}\")\nprint(f\"Number of testing data: {len(test_data)}\")\n\ntrain_data_len = len(train_data)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.398281Z","iopub.execute_input":"2021-05-22T18:24:11.398675Z","iopub.status.idle":"2021-05-22T18:24:11.404208Z","shell.execute_reply.started":"2021-05-22T18:24:11.398638Z","shell.execute_reply":"2021-05-22T18:24:11.403119Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Number of training data: 79915\nNumber of validation data: 7990\nNumber of testing data: 7990\n","output_type":"stream"}]},{"cell_type":"code","source":"print(vars(train_data.examples[0]))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.405908Z","iopub.execute_input":"2021-05-22T18:24:11.406303Z","iopub.status.idle":"2021-05-22T18:24:11.422530Z","shell.execute_reply.started":"2021-05-22T18:24:11.406248Z","shell.execute_reply":"2021-05-22T18:24:11.421625Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"{'label': 'neutral', 'sequence': [101, 1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012, 102, 1037, 2711, 2003, 2731, 2010, 3586, 2005, 1037, 2971, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(vars(train_data.examples[0])['sequence']))\nprint(len(vars(train_data.examples[0])['attention_mask']))\nprint(len(vars(train_data.examples[0])['token_type']))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.423773Z","iopub.execute_input":"2021-05-22T18:24:11.424112Z","iopub.status.idle":"2021-05-22T18:24:11.434431Z","shell.execute_reply.started":"2021-05-22T18:24:11.424079Z","shell.execute_reply":"2021-05-22T18:24:11.433431Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"25\n25\n25\n","output_type":"stream"}]},{"cell_type":"code","source":"tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[0])['sequence'])\n\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.436044Z","iopub.execute_input":"2021-05-22T18:24:11.436633Z","iopub.status.idle":"2021-05-22T18:24:11.450091Z","shell.execute_reply.started":"2021-05-22T18:24:11.436585Z","shell.execute_reply":"2021-05-22T18:24:11.449103Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"['[CLS]', 'a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.', '[SEP]', 'a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.', '[SEP]']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(vars(valid_data.examples[0]))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.451511Z","iopub.execute_input":"2021-05-22T18:24:11.451945Z","iopub.status.idle":"2021-05-22T18:24:11.462452Z","shell.execute_reply.started":"2021-05-22T18:24:11.451903Z","shell.execute_reply":"2021-05-22T18:24:11.461407Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"{'label': 'neutral', 'sequence': [101, 1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012, 102, 1037, 2711, 2003, 2731, 2010, 3586, 2005, 1037, 2971, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"code","source":"tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[0])['sequence'])\n\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.463698Z","iopub.execute_input":"2021-05-22T18:24:11.464124Z","iopub.status.idle":"2021-05-22T18:24:11.475730Z","shell.execute_reply.started":"2021-05-22T18:24:11.464087Z","shell.execute_reply":"2021-05-22T18:24:11.474793Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"['[CLS]', 'a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.', '[SEP]', 'a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.', '[SEP]']\n","output_type":"stream"}]},{"cell_type":"code","source":"LABEL.build_vocab(train_data)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.478956Z","iopub.execute_input":"2021-05-22T18:24:11.479441Z","iopub.status.idle":"2021-05-22T18:24:11.669670Z","shell.execute_reply.started":"2021-05-22T18:24:11.479400Z","shell.execute_reply":"2021-05-22T18:24:11.668816Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"print(LABEL.vocab.stoi)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.671820Z","iopub.execute_input":"2021-05-22T18:24:11.672345Z","iopub.status.idle":"2021-05-22T18:24:11.677602Z","shell.execute_reply.started":"2021-05-22T18:24:11.672306Z","shell.execute_reply":"2021-05-22T18:24:11.676583Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"defaultdict(None, {'entailment': 0, 'contradiction': 1, 'neutral': 2})\n","output_type":"stream"}]},{"cell_type":"code","source":"print(LABEL.vocab.freqs.most_common())","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.679075Z","iopub.execute_input":"2021-05-22T18:24:11.679688Z","iopub.status.idle":"2021-05-22T18:24:11.687912Z","shell.execute_reply.started":"2021-05-22T18:24:11.679644Z","shell.execute_reply":"2021-05-22T18:24:11.686927Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"[('entailment', 26697), ('contradiction', 26647), ('neutral', 26571)]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(LABEL.vocab.itos)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.689280Z","iopub.execute_input":"2021-05-22T18:24:11.689712Z","iopub.status.idle":"2021-05-22T18:24:11.699937Z","shell.execute_reply.started":"2021-05-22T18:24:11.689654Z","shell.execute_reply":"2021-05-22T18:24:11.699053Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"['entailment', 'contradiction', 'neutral']\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE = 16\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size = BATCH_SIZE,\n    sort_key = lambda x: len(x.sequence),\n    sort_within_batch = False, \n    device = device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.701745Z","iopub.execute_input":"2021-05-22T18:24:11.702103Z","iopub.status.idle":"2021-05-22T18:24:11.731022Z","shell.execute_reply.started":"2021-05-22T18:24:11.702072Z","shell.execute_reply":"2021-05-22T18:24:11.730049Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertModel\n\nbert_model = BertModel.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:11.732527Z","iopub.execute_input":"2021-05-22T18:24:11.733192Z","iopub.status.idle":"2021-05-22T18:24:36.845015Z","shell.execute_reply.started":"2021-05-22T18:24:11.733148Z","shell.execute_reply":"2021-05-22T18:24:36.844145Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a80339df494b7d881ef1d04dd1ba06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2014dd54786644bda548d16a1d092430"}},"metadata":{}}]},{"cell_type":"code","source":"print(device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:36.846281Z","iopub.execute_input":"2021-05-22T18:24:36.846630Z","iopub.status.idle":"2021-05-22T18:24:36.854647Z","shell.execute_reply.started":"2021-05-22T18:24:36.846594Z","shell.execute_reply":"2021-05-22T18:24:36.853810Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass BERTNLIModel(nn.Module):\n    def __init__(self,\n                 bert_model,\n                 hidden_dim,\n                 output_dim,\n                ):\n        \n        super().__init__()\n        \n        self.bert = bert_model\n        \n        embedding_dim = bert_model.config.to_dict()['hidden_size']\n        \n        #self.fc = nn.Linear(embedding_dim, hidden_dim)\n\n        #self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n\n        self.out = nn.Linear(embedding_dim, output_dim)\n        \n        \n    def forward(self, sequence, attn_mask, token_type):\n        \n        #sequence = [sequence len, batch_size]\n        #attention_mask = [seq_len, batch_size]\n        #token_type = [seq_len, batch_size]\n                \n        embedded = self.bert(input_ids = sequence, attention_mask = attn_mask, token_type_ids= token_type)[1]\n        #print('emb ', embedded.size())\n\n        #self.bert() gives tuple which contains hidden outut corresponding to each token.\n        #self.bert()[0] = [seq_len, batch_size, emd_dim]\n                \n        #embedded = [batch size, emb dim]\n        \n        #ff = self.fc(embedded)\n        #ff = [batch size, hid dim]\n\n        #ff1 = self.fc2(ff)\n                \n        \n        \n        output = self.out(embedded)\n        #print('output: ', output.size())\n        #output = [batch size, out dim]\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:36.855932Z","iopub.execute_input":"2021-05-22T18:24:36.856437Z","iopub.status.idle":"2021-05-22T18:24:36.866563Z","shell.execute_reply.started":"2021-05-22T18:24:36.856399Z","shell.execute_reply":"2021-05-22T18:24:36.865841Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"HIDDEN_DIM = 512\nOUTPUT_DIM = len(LABEL.vocab)\n\nmodel = BERTNLIModel(bert_model,\n                         HIDDEN_DIM,\n                         OUTPUT_DIM,\n                        ).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:36.867698Z","iopub.execute_input":"2021-05-22T18:24:36.868235Z","iopub.status.idle":"2021-05-22T18:24:41.276060Z","shell.execute_reply.started":"2021-05-22T18:24:36.868195Z","shell.execute_reply":"2021-05-22T18:24:41.275145Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:41.277835Z","iopub.execute_input":"2021-05-22T18:24:41.278406Z","iopub.status.idle":"2021-05-22T18:24:41.288627Z","shell.execute_reply.started":"2021-05-22T18:24:41.278340Z","shell.execute_reply":"2021-05-22T18:24:41.287809Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"The model has 109,484,547 trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"#for name, param in model.named_parameters():                \n#    if name.startswith('bert'):\n#        param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:41.289906Z","iopub.execute_input":"2021-05-22T18:24:41.290248Z","iopub.status.idle":"2021-05-22T18:24:41.297630Z","shell.execute_reply.started":"2021-05-22T18:24:41.290211Z","shell.execute_reply":"2021-05-22T18:24:41.296604Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"print(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:41.299434Z","iopub.execute_input":"2021-05-22T18:24:41.299684Z","iopub.status.idle":"2021-05-22T18:24:41.310715Z","shell.execute_reply.started":"2021-05-22T18:24:41.299658Z","shell.execute_reply":"2021-05-22T18:24:41.309774Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"The model has 109,484,547 trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import *","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:41.312320Z","iopub.execute_input":"2021-05-22T18:24:41.312761Z","iopub.status.idle":"2021-05-22T18:24:48.156690Z","shell.execute_reply.started":"2021-05-22T18:24:41.312719Z","shell.execute_reply":"2021-05-22T18:24:48.154976Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n  '\"sox\" backend is being deprecated. '\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.optim as optim\n\n#optimizer = optim.Adam(model.parameters())\noptimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-6,correct_bias=False)\n\ndef get_scheduler(optimizer, warmup_steps):\n    scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:48.158473Z","iopub.execute_input":"2021-05-22T18:24:48.158884Z","iopub.status.idle":"2021-05-22T18:24:48.170838Z","shell.execute_reply.started":"2021-05-22T18:24:48.158822Z","shell.execute_reply":"2021-05-22T18:24:48.167735Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss().to(device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:48.172358Z","iopub.execute_input":"2021-05-22T18:24:48.172752Z","iopub.status.idle":"2021-05-22T18:24:49.688801Z","shell.execute_reply.started":"2021-05-22T18:24:48.172711Z","shell.execute_reply":"2021-05-22T18:24:49.687709Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def categorical_accuracy(preds, y):\n    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n    correct = (max_preds.squeeze(1)==y).float()\n    return correct.sum() / len(y)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:49.690480Z","iopub.execute_input":"2021-05-22T18:24:49.691979Z","iopub.status.idle":"2021-05-22T18:24:49.700224Z","shell.execute_reply.started":"2021-05-22T18:24:49.691928Z","shell.execute_reply":"2021-05-22T18:24:49.699269Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"fp16 = True\n\nif fp16:\n    try:\n        from apex import amp\n    except ImportError:\n        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n    model, optimizer = amp.initialize(model, optimizer, opt_level='O1')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:49.703241Z","iopub.execute_input":"2021-05-22T18:24:49.703623Z","iopub.status.idle":"2021-05-22T18:24:51.140407Z","shell.execute_reply.started":"2021-05-22T18:24:49.703569Z","shell.execute_reply":"2021-05-22T18:24:51.139544Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n\nDefaults for this optimization level are:\nenabled                : True\nopt_level              : O1\ncast_model_type        : None\npatch_torch_functions  : True\nkeep_batchnorm_fp32    : None\nmaster_weights         : None\nloss_scale             : dynamic\nProcessing user overrides (additional kwargs that are not None)...\nAfter processing overrides, optimization options are:\nenabled                : True\nopt_level              : O1\ncast_model_type        : None\npatch_torch_functions  : True\nkeep_batchnorm_fp32    : None\nmaster_weights         : None\nloss_scale             : dynamic\n","output_type":"stream"}]},{"cell_type":"code","source":"max_grad_norm = 1\n\ndef train(model, iterator, optimizer, criterion, scheduler):\n    #print(iterator)\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n\n        optimizer.zero_grad() # clear gradients first\n        torch.cuda.empty_cache() # releases all unoccupied cached memory \n        \n\n        sequence = batch.sequence\n        attn_mask = batch.attention_mask\n        token_type = batch.token_type\n        #print(sequence.size(), attn_mask.size(), token_type.size())\n        #print(sequence[0])\n        #print(attn_mask[0])\n        #print(token_type[0])\n        label = batch.label\n        \n        predictions = model(sequence, attn_mask, token_type)\n        \n        #predictions = [batch_size, 3]\n        #print(predictions.size())\n        \n        loss = criterion(predictions, label)\n        \n        acc = categorical_accuracy(predictions, label)\n        \n        if fp16:\n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n        else:\n            loss.backward()\n        \n        optimizer.step()\n        scheduler.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:51.141635Z","iopub.execute_input":"2021-05-22T18:24:51.141968Z","iopub.status.idle":"2021-05-22T18:24:51.152037Z","shell.execute_reply.started":"2021-05-22T18:24:51.141934Z","shell.execute_reply":"2021-05-22T18:24:51.151084Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    #print(iterator)\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n            #print(batch)\n\n            sequence = batch.sequence\n            attn_mask = batch.attention_mask\n            token_type = batch.token_type\n            labels = batch.label\n                        \n            predictions = model(sequence, attn_mask, token_type)\n            \n            loss = criterion(predictions, labels)\n                \n            acc = categorical_accuracy(predictions, labels)\n            \n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:51.153318Z","iopub.execute_input":"2021-05-22T18:24:51.153732Z","iopub.status.idle":"2021-05-22T18:24:51.166942Z","shell.execute_reply.started":"2021-05-22T18:24:51.153685Z","shell.execute_reply":"2021-05-22T18:24:51.166060Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:51.169166Z","iopub.execute_input":"2021-05-22T18:24:51.170246Z","iopub.status.idle":"2021-05-22T18:24:51.181751Z","shell.execute_reply.started":"2021-05-22T18:24:51.170216Z","shell.execute_reply":"2021-05-22T18:24:51.181031Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nN_EPOCHS = 6\n\nwarmup_percent = 0.2\ntotal_steps = math.ceil(N_EPOCHS*train_data_len*1./BATCH_SIZE)\nwarmup_steps = int(total_steps*warmup_percent)\nscheduler = get_scheduler(optimizer, warmup_steps)\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, scheduler)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    \n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'bert-nli.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T18:24:51.183111Z","iopub.execute_input":"2021-05-22T18:24:51.183483Z","iopub.status.idle":"2021-05-22T19:43:27.258977Z","shell.execute_reply.started":"2021-05-22T18:24:51.183445Z","shell.execute_reply":"2021-05-22T19:43:27.257798Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\nEpoch: 01 | Epoch Time: 13m 9s\n\tTrain Loss: 0.624 | Train Acc: 73.48%\n\t Val. Loss: 0.363 |  Val. Acc: 87.06%\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\nEpoch: 02 | Epoch Time: 13m 10s\n\tTrain Loss: 0.399 | Train Acc: 85.44%\n\t Val. Loss: 0.236 |  Val. Acc: 91.95%\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\nEpoch: 03 | Epoch Time: 13m 7s\n\tTrain Loss: 0.288 | Train Acc: 90.30%\n\t Val. Loss: 0.155 |  Val. Acc: 95.10%\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\nEpoch: 04 | Epoch Time: 13m 5s\n\tTrain Loss: 0.221 | Train Acc: 93.18%\n\t Val. Loss: 0.107 |  Val. Acc: 96.91%\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\nEpoch: 05 | Epoch Time: 13m 1s\n\tTrain Loss: 0.181 | Train Acc: 94.87%\n\t Val. Loss: 0.101 |  Val. Acc: 97.05%\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\nGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\nEpoch: 06 | Epoch Time: 12m 54s\n\tTrain Loss: 0.149 | Train Acc: 96.05%\n\t Val. Loss: 0.074 |  Val. Acc: 98.02%\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load('bert-nli.pt'))\n\ntest_loss, test_acc = evaluate(model, test_iterator, criterion)\n\nprint(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:43:27.263388Z","iopub.execute_input":"2021-05-22T19:43:27.265736Z","iopub.status.idle":"2021-05-22T19:43:39.283837Z","shell.execute_reply.started":"2021-05-22T19:43:27.265691Z","shell.execute_reply":"2021-05-22T19:43:39.282313Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Test Loss: 0.074 |  Test Acc: 98.02%\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_inference(premise, hypothesis, model, device):\n    \n    model.eval()\n    \n    premise = '[CLS] ' + premise + ' [SEP]'\n    hypothesis = hypothesis + ' [SEP]'\n    \n    prem_t = tokenize_bert(premise)\n    hypo_t = tokenize_bert(hypothesis)\n    \n    #print(len(prem_t), len(hypo_t))\n    \n    prem_type = get_sent1_token_type(prem_t)\n    hypo_type = get_sent2_token_type(hypo_t)\n    \n    #print(len(prem_type), len(hypo_type))\n    \n    indexes = prem_t + hypo_t\n    \n    indexes = tokenizer.convert_tokens_to_ids(indexes)\n    #print(indexes)\n    indexes_type = prem_type + hypo_type\n    #print(indexes_type)\n    \n    attn_mask = get_sent2_token_type(indexes)\n    #print(attn_mask)\n    \n    #print(len(indexes))\n    #print(len(indexes_type))\n    #print(len(attn_mask))\n\n    #seq = '[CLS] '+ premise + ' [SEP] '+ hypothesis \n\n    #tokens = tokenizer.tokenize(seq)\n\n    #indexes = tokenizer.convert_tokens_to_ids(tokens)\n    \n    indexes = torch.LongTensor(indexes).unsqueeze(0).to(device)\n    indexes_type = torch.LongTensor(indexes_type).unsqueeze(0).to(device)\n    attn_mask = torch.LongTensor(attn_mask).unsqueeze(0).to(device)\n    \n    #print(indexes.size())\n    \n    prediction = model(indexes, attn_mask, indexes_type)\n    \n    prediction = prediction.argmax(dim=-1).item()\n    \n    return LABEL.vocab.itos[prediction]","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:43:39.285211Z","iopub.execute_input":"2021-05-22T19:43:39.285573Z","iopub.status.idle":"2021-05-22T19:43:39.293377Z","shell.execute_reply.started":"2021-05-22T19:43:39.285534Z","shell.execute_reply":"2021-05-22T19:43:39.292238Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"premise = 'a man sitting on a green bench.'\nhypothesis = 'a woman sitting on a green bench.'\n\npredict_inference(premise, hypothesis, model, device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:43:39.294655Z","iopub.execute_input":"2021-05-22T19:43:39.295190Z","iopub.status.idle":"2021-05-22T19:43:39.345334Z","shell.execute_reply.started":"2021-05-22T19:43:39.295145Z","shell.execute_reply":"2021-05-22T19:43:39.344616Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"'contradiction'"},"metadata":{}}]},{"cell_type":"code","source":"premise = 'a man sitting on a green bench.'\nhypothesis = 'a man sitting on a blue bench.'\n\npredict_inference(premise, hypothesis, model, device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:43:39.346465Z","iopub.execute_input":"2021-05-22T19:43:39.346775Z","iopub.status.idle":"2021-05-22T19:43:39.382134Z","shell.execute_reply.started":"2021-05-22T19:43:39.346742Z","shell.execute_reply":"2021-05-22T19:43:39.381216Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"'contradiction'"},"metadata":{}}]},{"cell_type":"code","source":"premise = 'I am lying down on bed.'\nhypothesis = 'I am resting on bed.'\n\npredict_inference(premise, hypothesis, model, device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:43:39.383222Z","iopub.execute_input":"2021-05-22T19:43:39.383665Z","iopub.status.idle":"2021-05-22T19:43:39.421339Z","shell.execute_reply.started":"2021-05-22T19:43:39.383628Z","shell.execute_reply":"2021-05-22T19:43:39.420435Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"'entailment'"},"metadata":{}}]},{"cell_type":"code","source":"premise = 'I go to office on my personal car.'\nhypothesis = 'I have to share office cab for reaching office.'\n\npredict_inference(premise, hypothesis, model, device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:43:39.422429Z","iopub.execute_input":"2021-05-22T19:43:39.422755Z","iopub.status.idle":"2021-05-22T19:43:39.458545Z","shell.execute_reply.started":"2021-05-22T19:43:39.422722Z","shell.execute_reply":"2021-05-22T19:43:39.457617Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"'neutral'"},"metadata":{}}]},{"cell_type":"code","source":"premise = 'I love to play cricket.'\nhypothesis = 'I enjoy playing football.'\n\npredict_inference(premise, hypothesis, model, device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:43:39.459640Z","iopub.execute_input":"2021-05-22T19:43:39.459985Z","iopub.status.idle":"2021-05-22T19:43:39.498287Z","shell.execute_reply.started":"2021-05-22T19:43:39.459951Z","shell.execute_reply":"2021-05-22T19:43:39.497356Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"'contradiction'"},"metadata":{}}]},{"cell_type":"code","source":"premise = 'He is techy.'\nhypothesis = 'He has no idea of tech.'\n\npredict_inference(premise, hypothesis, model, device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:43:39.499376Z","iopub.execute_input":"2021-05-22T19:43:39.499708Z","iopub.status.idle":"2021-05-22T19:43:39.537532Z","shell.execute_reply.started":"2021-05-22T19:43:39.499675Z","shell.execute_reply":"2021-05-22T19:43:39.536636Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"'contradiction'"},"metadata":{}}]},{"cell_type":"code","source":"premise = 'I am using mobile phone.'\nhypothesis = 'I have mobile in my hand.'\n\npredict_inference(premise, hypothesis, model, device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:43:39.538597Z","iopub.execute_input":"2021-05-22T19:43:39.539065Z","iopub.status.idle":"2021-05-22T19:43:39.585624Z","shell.execute_reply.started":"2021-05-22T19:43:39.539029Z","shell.execute_reply":"2021-05-22T19:43:39.584727Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"'entailment'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}